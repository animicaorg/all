# Benchmark Report (human-readable)

> This file is a **human-friendly** summary of the last benchmark run.  
> It can be regenerated by running the benchmark runner with the `--md` flag:
>
> ```bash
> python tests/bench/runner.py \
>   --patterns "da/bench/*.py" "mining/bench/*.py" "randomness/bench/*.py" \
>   --save benchmarks/current.json \
>   --compare tests/bench/baselines.json \
>   --threshold-pct 15 \
>   --md tests/bench/report.md
> ```

---

## How to interpret the table

- **Metric**:
  - `ops_per_s`: higher is better.
  - `median_s`/`mean_s`: lower is better.
- **Δ%**: positive means an improvement in the preferred direction (see metric).  
- **Status**: quick classification (improve/regress/same/new/missing).
- The runner exits non-zero if any regression exceeds the configured threshold (default **15%**).

> Tip: On Linux, consider pinning to a CPU for stability:
> ```bash
> taskset -c 2 python tests/bench/runner.py ... --cpu 2
> ```

---

## Environment (from last run)

- Prev git: *(not recorded yet)*
- Cur git: *(not recorded yet)*

> The runner fills these fields when it writes this report.

---

## Results

| Case | Metric | Prev | Cur | Δ% | Status |
|------|--------|------|-----|----|--------|
| *(no runs yet)* | — | — | — | — | — |

---

## Notes

- Canonical comparison baselines live in: `tests/bench/baselines.json`.  
  Update them via PR **only** when a performance change is expected or desired.
- Raw JSON for the last run is saved by the runner (see `--save` path).
- If some scripts print extra logs, the runner attempts to extract the **last** JSON object from stdout.

---
